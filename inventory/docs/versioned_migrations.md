<!---
  SPDX-FileCopyrightText: (C) 2025 Intel Corporation
  SPDX-License-Identifier: Apache-2.0
-->

# Database Schema Migration

The inventory service uses the [entgo.io] object-relational mapping (ORM), which supports automatic
and versioned migration.

Automatic migration is easy but is limited to relatively simple
schema changes. To provide a more robust migration capabilities, the
inventory service uses the version migration approach, which is facilitated
by ent ORM and [Atlas\* tool] - an open-source database schema management system.

Both ent and Atlas tool work together to provide the tooling for projects to make
versioned schema migrations relatively easy and streamlined. There are two
high-level stages in this process:

1) Generate migrations (at development time)
2) Apply migrations (at deployment time)

The ent ORM framework supports database schema generation from Go\* code or Protobuf,
which also allows it to generate change records that track how the schema
changes from the generated versioned migration files.

## Generate Migrations

There are several phases in this stage. Each is briefly described in the
sections below in concrete terms of how it applies to the Inventory service
project.

### Modify the Databases Schema

When the developer makes changes to the Protobuf schema definitions, they will
usually run the following to regenerate the ent schema and Go code:

```bash
make buf-gen && make ent-gen
```

After that, other code changes can be made to implement a new feature or fix.

### Generate Migration Scripts or Code

Once the code modifications are done, the developer also needs to generate
an SQL migration accompanying the schema change, before submitting a patch.
This is accomplished using the following command, where the `migration-name`
parameter gives the aggregated set of schema changes a name.

```bash
make migration-generate MIGRATION=<migration-name>
```

Running this command brings up a temporary postgres (PostgreSQL\* database server) database instance,
which will be used to replay the entire migration history to compute the current
schema state and to produce a set of SQL commands to migrate the schema to the
new or future state. These commands, along with hashes, will be stored in
the `internal/ent/migrate/migrations` directory, effectively extending the
schema evolution records and thus supporting future migrations. The generated
migration records and the generated SQL commands must be checked-in, as
they become part of the Docker\* image.

### Data-dependent Migrations

Sometimes, the migrations automatically generated in the previous step are not
sufficient to actually migrate a live database with data in it. The [linter](#validate-generated-scripts)
must catch and highlight those cases. Then the developer has to either

1. manually modify the auto-generated SQL statements or create new ones
2. write a data migration in Go code

#### Manually Code a Migration from Scratch or Modify an Existing One

If the autogenerated schema migration is not too far off and only data conversion
needs to happen, the existing SQL statement can be modified or amended with
manually written SQL code. This can involve adding a `USING <expression>`
statement to the `ALTER TABLE` clause to facilitate data conversion. See:

- [https://stackoverflow.com/q/19559851](https://stackoverflow.com/q/19559851)
- [https://stackoverflow.com/questions/7162903/how-to-alter-a-columns-data-type-in-a-postgresql-table#comment35877191_7162961](https://stackoverflow.com/questions/7162903/how-to-alter-a-columns-data-type-in-a-postgresql-table#comment35877191_7162961
)
- [https://www.postgresql.org/docs/14/sql-altertable.html](https://www.postgresql.org/docs/14/sql-altertable.html)
- [https://entgo.io/docs/data-migrations#versioned-migrations](https://entgo.io/docs/data-migrations#versioned-migrations)
- [https://atlasgo.io/lint/analyzers](https://atlasgo.io/lint/analyzers)
- [https://atlasgo.io/versioned/troubleshoot](https://atlasgo.io/versioned/troubleshoot)

> Example - adding a `UNIQUE INDEX` to a non-unique column

For example, if you want to add a unique constraint to a previously unconstrained
column or field, Atlas will generate a suitable schema transformation that
will work on an empty database:

```sql
-- Create index "host_resources_device_guid_key" to table: "host_resources"
CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
```

But because non-unique data has been added to a production database, that same
migration will fail to apply in the prod setup:

```sql
postgres=# CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
ERROR:  could not create unique index "host_resources_device_guid_key"
DETAIL:  Key (device_guid)=(guid-1) is duplicated.
```

These duplicate values need to be removed or otherwise consolidated, before a
unique index can be created. In this instance, find them and set all of them
to NULL by adding an additional SQL statement to the generated migration:

```sql
-- Set duplicate "device_guid" rows to NULL.
UPDATE "host_resources" a SET "device_guid" = NULL FROM "host_resources" b WHERE a.id < b.id AND a.device_guid = b.device_guid;
-- Create index "host_resources_device_guid_key" to table: "host_resources"
CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
```

If such a modification is not be feasible, a migration can be written entirely
from scratch too:

```bash
make migration-new MIGRATION=<migration-name>
```

**Remember to update the hash file when done as [described](#re-hash-the-atlassum-file).**

#### Generate a Migration from the Go Code

Instead of writing SQL statements like the one shown above, developers can write Go code
leveraging the ent-provided interface to describe the desired modifications
instead. The ent diff planner will then generate the SQL statements from that:
[https://entgo.io/docs/data-migrations#generated-scripts](https://entgo.io/docs/data-migrations#generated-scripts). See the
[driver code](/internal/ent/migrate/gen_code_migrations.go) for further
instructions and an example.

### Re-hash the `atlas.sum` File

Whenever migrations are manually modified, added, or removed, the `atlas.sum`
checksum file needs to be updated to reflect the modified state. This can be
triggered with the following make target:

```bash
make migration-hash
```

### Validate Generated Scripts

The Atlas tool also includes a linter that can validate the
generated migration scripts and can alert the developer or
database admin about possible problems in the upcoming migration. To run this
validation, execute this command:

```bash
make migration-lint
```

The output of this command may include warning about potential issues, which may
need to be addressed by additional manually written code. If schema changes are
made deliberately, this should not be necessary, but sometimes it may not be
avoidable.

At this point, the developer or database administrator has all the collateral to apply
(or execute) the migration.

## Apply Migrations

A newer version of the inventory service will always attempt to migrate the
given database at startup, using migration files provided with the required
`-migrationsDir` flag. In most cases, this is simply a no-op and will not change
the schema at all. This process will abort if any errors are detected. In this
failed state, manual human intervention is required.
Later in the startup process, the inventory will check that the database schema
is congruent with the expected ent schema. If this is not the case, inventory
will not start.

Before applying the migration on a production system, it is a good idea to first
do a dry-run of the migration scripts on an off-line copy of the production
system to ensure there are no unexpected issues, which could
cause the migration to leave the production system in an unusable state.

Ent automatic migrations, not to be confused with the automatic application of
migrations, and versioned migrations are fundamentally incompatible and will
lead to a broken database state when mixed together. **Hence, the inventory only uses
versioned migrations.**

## More Documentation

More detailed documentation is provided on [entgo.io versioned migrations] page.
The above page provides a summary and a distillation of concrete steps to
be executed in the context of this specific project and database drivers.

[Atlas\* tool]: https://atlasgo.io/getting-started/
[entgo.io]: https://entgo.io
[entgo.io versioned migrations]: https://entgo.io/docs/versioned-migrations/

## Troubleshooting

If you have an incompatible change like the following:

```bash
20230612170303_ci-mig.sql: destructive changes detected:

	L4: Dropping non-virtual column "target"

20230612170303_ci-mig.sql: data dependent changes detected:

	L12: Adding a unique index "repeated_schedule_resources_resource_id_key" on table "repeated_schedule_resources" might fail in case column "resource_id" contains duplicate entries
```

You can look at the current change that is causing the issue. Then you can
leverage the `atlas.sum` to have the order list of the migrations. Note that
each migration is a diff on the schema. Removing the incompatible change will
solve the issue. To do that, you must first find the migration file that is
incompatible, then you remove or modify the file.

**Remember to update the hash file when done as [described](#re-hash-the-atlassum-file).**

Then, regenerate the new migration and lint the migration files together.

### Deleting the Schema and All Data

In some cases, the database is irreparably modified and cannot be recovered. Then
it has to be cleaned and the schema can be created from scratch. Atlas tool provides
a helper function to facilitate database cleanup.

#### In a Local Setup

If there is a running inventory instance, run the Atlas binary inside it:

```bash
docker exec -ti <inventory container> atlas schema clean --url "postgres://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE?search_path=public&sslmode=$PGSSLMODE"
```

If there is no running container, start a new one and run the Atlas binary directly:

```bash
docker run --rm -ti --net=host --env PGUSER=admin --env PGPASSWORD=pass --env PGSSLMODE=disable --env PGPORT=5432 --env PGHOST=localhost --env PGDATABASE=postgres --entrypoint atlas [INFRA INVENTROY DOCKER IMAGE]:main schema clean --url "postgres://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE?search_path=public&sslmode=$PGSSLMODE"
```

#### In a Production Setup

In production, you can derive the data needed for the access from the Kubernetes\* secret.

```bash
PGDATA=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGDATABASE}'| base64 -d)
PGHOST=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGHOST}'| base64 -d)
PGPSWD=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGPASSWORD}'| base64 -d)
PGUSER=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGUSER}'| base64 -d)
PGPORT=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGPORT}'| base64 -d)

kubectl exec -it -n orch-infra infra-inventory-0 -- atlas schema clean \
  --url "postgres://$PGUSER:$PGPWD@$PGHOST:$PGPORT/$PGDB?search_path=public&sslmode=require"
```

**This will delete all stored data!**

### Access the Database in a Production Setup

1. Derive the required data for accessing the database, from Kubernetes secrets:

    ```bash
    PGDATA=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGDATABASE}'| base64 -d)
    PGHOST=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGHOST}'| base64 -d)
    PGPSWD=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGPASSWORD}'| base64 -d)
    PGUSER=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGUSER}'| base64 -d)
    PGPORT=$(kubectl get secret -n orch-infra inventory-aurora-postgresql -o jsonpath='{.data.PGPORT}'| base64 -d)
    ```

2. Run a Postgres container in the cluster:

    ```bash
   kubectl run -n orch-infra psql --image=postgres:14.6-alpine --restart=Never -i -q --rm -- sleep infinity >/dev/null 2>&1 &
   ```

3. Run a psql shell in the Postgres container connecting to the actual postgres database (Amazon Aurora\* database or
platform postgres database):

    ```bash
    kubectl exec -n orch-infra -it psql -- env PGPASSWORD="$PGPSWD" psql -h "$PGHOST" -p "$PGPORT" -U "$PGUSER" -d "$PGDATA"
    ```

   This will spawn a psql shell with full access to the database.

   **Note: Any query in psql is done directy against the database**

4. Run any psql command, for example:

   ```bash
   orch-infra-inventory=> \d
   List of relations
   Schema |                Name                |   Type   |               Owner
   --------+------------------------------------+----------+------------------------------------
   public | atlas_schema_revisions             | table    | orch-infra-inventory_user
   public | endpoint_resources                 | table    | orch-infra-inventory_user
   public | endpoint_resources_id_seq          | sequence | orch-infra-inventory_user
   ...
   ```

5. Run any SQL query, for example:

   ```bash
   orch-infra-inventory=> SELECT * FROM instance_resources;
   id |  resource_id  |        kind         |      description       |     desired_state      | current_state | vm_memory_bytes | vm_cpu_cores | vm_storage_bytes | instance_resource_host | instance_resource_user | instance_resource_os | status | status_detail
   ----+---------------+---------------------+------------------------+------------------------+---------------+-----------------+--------------+------------------+------------------------+------------------------+----------------------+--------+---------------
   1 | inst-fb4e21ad | INSTANCE_KIND_METAL | host-03a45e18-instance | INSTANCE_STATE_RUNNING |               |                 |              |                  |                      5 |                        |                    1 |        |
   (1 row)
   ```

6. Remember to delete the psql pod:

   ```bash
   kubectl delete pods -n orch-infra psql
   ```
